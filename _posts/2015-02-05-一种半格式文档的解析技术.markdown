---
layout: post
title:  "一种半格式文档的解析技术"
date:   2015-02-05
---
一种半结构化文档解析技术

背景介绍  
     
	经过一些简单的数据分析，不难发现邮箱的用户全体中，每天有不少的用户在利用邮箱发送简历，既然发送简历的需求这么多，为了方便用户更好的管理自己的简历，在QQ邮中增加简历中心的功能就很有必要了。
	既然需要增加简历中心的功能，那么对用户已经写好的简历文档，智能的分析用户的文档，然后写入到简历中心的信息库中（也就是智能导入简历）的功能就不可或缺了。

技术选型

	要做智能导入简历功能的话，第一步要明白我要导入的对象有那些特点，从现在邮箱已经存在的简历中抽样一些简历进行分析。

通过抽样分析，可以发现简历文档是具有一定的格式的

如下：
 

可以简单的得出结论：
简历是一种典型的半格式化的文本，其文本组织方式与普通文本不同，具有一定的格式，但与严格的格式文本（如XML）不同，这种格式是相对的，比较松散，具有一定的自由度。

而且可以发现简历的一个突出特点，简历文档中出现的词语范围是比较固定的，比如说会经常出现“工作经历”、“教育经验”、“个人信息”，“联系方式”等词，而且这些词，本身除了自己的词语本意以外，还有组织文档结构的作用，这也是我们说它是半格式化文档原因。

	既然简历文档有这么一个特别，那么如何利用这个特点进行简历分析呢？我们首先想到的就是分词技术。
	如上所述，简历中的词语本身就有组织文档结构功能，把简历分解成一个一个词语，然后根据对简历的词语的统计，赋予简历中的词语对应的词性和权重，（分析现网近6w封简历，得出经常出现在简历中的词语组成的词库，然后再人工给这些词赋予对应的词性和权重。分析方法见附录）。根据这些词的词性，就可以找到我们需要的简历信息了。


解析算法
	上面讲了那么多，那具体是是如何做的呢？
	算法基本流程：

第一步：
	由于简历的文档格式可能是pdf、doc、html、txt等不同形式，为了后面处理的统一化，而且技术主要采用的是分词技术，所以我们统一在解析简历之前，把不同的格式都转换成txt格式。

第二步：
	文档分段：遍历这个txt文档，对整个文档分词，可以得到整个文档的词性权重信息表。比如下面这个简历：
	
	
以行做为最小单元组织信息。
大概可以得到这样一张词性权重信息表

行数	词	总权重
1	工作经历（词性：工作；权重：10）	10
2	2011-06(词性：时间；权重：1)
至（词性：普通；权重：1）
2012-09（词性：时间；权重：1） 
上海军惠数码科技有限公司（词性：工作；权重：3）
	6
3	IT部（词性：工作；权重：3） 
程序员（词性：工作；权重：3）
	6
4	工作描述（词性：工作；权重10）
主要（词性：普通；权重：1）
...
和（词性：普通；权重：1）
项目（词性：项目；权重：10）
前期（词性：普通；权重：1）
与（词性：普通；权重：1）
客户讨论。（词性：工作；权重：1）
	26
5	项目经验（词性：项目；权重：10）	10
6	项目描述（词性：项目；权重：10）
为了（词性：普通；权重：1）
对（词性：普通；权重：1）
已有（词性：普通；权重：1）
的（词性：普通；权重：1）
中医（词性：工作；权重：3）
...
健（词性：普通；权重：1）	32
7	康（词性：普通；权重：1）
干预（词性：普通；权重：1）
...
管理（词性：普通；权重：1）	20

得到整个文档的词性权重表后，再采用投票选举的算法来确定每一行的属性（普通词性的词没有被选举权）
每一个词性的得票率=

根据这个简单的算法很容易得出每一行中哪一种词性的得票率是最大的，那么这个最大的得票率的词性，就可以确定为这个行属性了。
如上面的例子：1、2、3、4行可以确定为工作属性
              5、6行可以确定为项目属性
              第7行无法确定属性，我们采取跟随上一行属性的做法，
	  那么它也是项目属性
经过这一番计算，就可以将上面的例子分成两段了
第一段，工作相关段


第二段，项目相关段


第三步：
	通过上一步的计算，我们已经将文档分好了段。但是有些属性段，还需要分块，
比如说上面的工作属性段。上面的这个例子中，工作经历只有一项，但是很多情况下
用户的工作经历可能有多项，这个时候就需要将工作属性段按照它的项进行分块。
	这里分块的的基本算法是怎样的呢？
	其实也很简单，我们通过属性段的项的特征词进行分块，什么叫特性词呢？特性词就是一个属性段中的一项必须或者最基本的应该包含的词，比如说这里举例的工作属性段，那么每一工作项应该都包含一个工作单位，那么利用这个工作单位进行分块，我们找到段中各工作单位所在的行，然后用这些行作为块的分隔标记。这样就可以把段分成块了。
	这里有个麻烦的地方就是，每一种属性段的特性词都是不同的，所以对不同的属性段
分块的时候，要实现不同的分块方法。

第四步：
	通过前面三步，我们已经得到了一块一块的信息，比如说上面的工作信息块
	
	
	
但是得到这样的信息块，还不能完全满足我们的要求，还需要将这其中具体的信息点
抽取出来，比如说这个例子，我们需要抽取 “工作时间”、“工作单位”、“工作职位”
“工作描述”等。
	再对这个信息块采用分词技术，但是这里采用的词库和第二步不同，这
个时候，我们分析工作属性的时候，采用的是工作属性词库。
	像第二步一样，这里又可以得到一个词性表。
词	词性
2011-06	时间
至	普通
2012-09	时间
上海军惠数码科技有限公司	工作单位
IT部 	工作部门
程序员	工作职位
工作描述	工作描述
主要	普通
...	普通
客户讨论	普通
	
通过这个词性表，就可以得到我们想要的“工作时间”、“工作单位”、“工作职位”
“工作描述”等信息了。

	这里我们使用了一些技巧，比如说工作单位“上海军惠数码科技有限公司”，这个词可能不在词库中，这个时候通过前缀、后缀的方式来找到公司信息，就拿这个例子来讲“上海军惠数码科技有限公司”这个词不在词库中，可以通过词库中的后缀“公司”来倒推出“上海军惠数码科技有限公司”是一个工作单位属性的词。

第五步：
	通过前面四步，简历文档真的大部分我们需要的信息都可以提取出来了，但是有些情况下，由于我们的分块或者分段错误，导致一些关键信息没有提出到，比如说姓名、联系方式信息，这个时候需要一个补充和完善的过程，我们就将整个文档当成一个信息块重走第四步。
	
效果评估
	通过上面的算法，得到了我们需要的信息，但是如何评估我们的算法的准确性呢？
我们想到了两种方式：
第一种，抽样比较。随机的抽取200封简历，然后人工抽取我们要的信息入库A，然后
再用我们的算法对这200封简历抽取，然后进行信息入库B。
 
准确率= （A、B相同信息个数）/ (A总数)

下面是我们的比较结果，这个比较系统也可以帮助优化算法。


第二种，线上跟踪。我们认为用户导入简历后，如果出现不准确的情况下用户会手工修改自己的简历，那么可以保存一份用户导入的简历信息B，然后再保存一份用户修改后的信息A。

准确率=（B、A相同的信息个数）/(A的总个数)

下面线上效果的跟踪结果

从图上可以看出算法还比较稳定，准确率保存在61%左右。

机器学习

从我们的解析算法中不难发现一点，那就是我们的算法中第二步和第四步非常依赖词库，词库丰满准确的话，算法的准确率也可以提升。

所以我们想到利用用户的完成填写的简历数据来丰富词库。

整个流程大概如下：



比如说用户的简历中公司这个词是“上海军惠数码科技有限公司”，拿到这个词，判断在词库记录中不？ 如果不在，加入到词库记录中,否则增加一次这个词的出现次数，然后判断这个次数是否达到了某个阀值（比如10次），如果达到，加入到解析词库中。



附录：
	
	词库的建立过程：
	
  分析大量的简历，就可以得到哪些词是经常出现在对应的属性中的，比如说“工作经历” 这个词一定会经常出现在描述工作对应的段落中，这样就初步建立起来了算法的初始词库了
